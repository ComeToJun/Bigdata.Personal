f = open("D:/JunPython/notepadd_workspace/change.html", encoding='utf-8')

html3 = f.read()
f.close()

soup = BeautifulSoup(html3, 'lxml')


title = soup.find('p', {"id": "title"})
contents = soup.find_all('p', {"id":"content"})

print(title.get_text())
for content in contents:
    print(content.get_text())

대한민국헌법
 제 1조  대한민국은 민주공화국이다.  대한민국의 주권은 국민에게 있고, 
모든 권력은 국민으로부터 나온다.
 제 2조  대한민국의 국민이 되는 요건은 법률로 정한다.  국가는 법률이 정하는 바에 의하여
재외국민을 보호할 의무를 진다.

------------------------------------------------------------------------------------------------------

def replace_newline(soup_html):
    br_to_newlines = soup_html.find_all("br")  <<-  **밑에 있는 문장의 결과물
    for br_to_newline in br_to_newlines:	      br 을 \n 으로 바꿔서 줄을 바꾸는 명령을 넣어준다.
        br_to_newline.replace_with("\n")
    return soup_html

f = open("D:/JunPython/notepadd_workspace/change.html", encoding='utf-8')

html3 = f.read()
f.close()

soup = BeautifulSoup(html3, 'lxml')


title = soup.find('p', {"id": "title"})
contents = soup.find_all('p', {"id":"content"})

print(title.get_text(), '\n')

for content in contents:
    content1 = replace_newline(content)
    print(content1.get_text(), '\n')


대한민국헌법 

 제 1조 
 대한민국은 민주공화국이다. 
 대한민국의 주권은 국민에게 있고, 
모든 권력은 국민으로부터 나온다. 

 제 2조 
 대한민국의 국민이 되는 요건은 법률로 정한다. 
 국가는 법률이 정하는 바에 의하여
재외국민을 보호할 의무를 진다. 

** Beautifulsoup 에서는 객체를 레퍼런스화 해서 참조하기 때문에 참조한 값을변화시키면
원본에도 영향을 줘서 변화시킨다. Beautifulsoup 특징이므로 기억해놓을 것.


---------------------------------------------------------------------------------------------------------------

url = 'https://www.alexa.com/topsites/countries/KR'

html_website_ranking = requests.get(url).text
soup_website_ranking = BeautifulSoup(html_website_ranking, 'lxml')

website_ranking = soup_website_ranking.select('p a')
# print(website_ranking[0:6])  #첫번째 값은 필요없다 는걸 결과값으로 알 수 있다.

website_ranking_site = [website_ranking_element.get_text()
                        for website_ranking_element in website_ranking[1:]]

print("[Top sites is South Korea]")
for k in range(len(website_ranking)-1):
    print("{0}: {1}".format(k + 1, website_ranking_site[k]))

[Top sites is South Korea]
1: Google.com
2: Naver.com
3: Youtube.com
4: Daum.net
5: Tistory.com
6: Tmall.com
7: Google.co.kr
8: Kakao.com
9: Amazon.com
10: Facebook.com
11: Sohu.com
12: Qq.com
13: Namu.wiki
14: Coupang.com
15: Netflix.com
16: Wikipedia.org
17: Login.tmall.com
18: Taobao.com
19: 360.cn
20: Jd.com
21: Baidu.com
22: Dcinside.com
23: Microsoft.com
24: Yahoo.com
25: Pages.tmall.com
26: Gmarket.co.kr
27: Twitch.tv
28: Apple.com
29: Instagram.com
30: Sina.com.cn
31: Weibo.com
32: Nexon.com
33: Bing.com
34: Donga.com
35: Adobe.com
36: Stackoverflow.com
37: Nate.com
38: 11st.co.kr
39: Office.com
40: Yna.co.kr
41: Ruliweb.com
42: Ebay.com
43: Joins.com
44: Theepochtimes.com
45: Auction.co.kr
46: Amazon.co.uk
47: Inven.co.kr
48: Dropbox.com
49: Chosun.com
50: Ppomppu.co.kr

-------------------------------------------------------------------------------------------------------------
import pandas as pd

url = 'https://www.alexa.com/topsites/countries/KR'

html_website_ranking = requests.get(url).text
soup_website_ranking = BeautifulSoup(html_website_ranking, 'lxml')

website_ranking = soup_website_ranking.select('p a')
# print(website_ranking[0:6])  #첫번째 값은 필요없다 는걸 결과값으로 알 수 있다.

website_ranking_sites = [website_ranking_element.get_text()
                        for website_ranking_element in website_ranking[1:]


website_ranking_dict = {'Website': website_ranking_sites}
df = pd.DataFrame(website_ranking_dict,
                  columns=['Website'], index=range(1,len(website_ranking_sites)+1))

print([df[0:6]])

[       Website
1   Google.com
2    Naver.com
3  Youtube.com
4     Daum.net
5  Tistory.com
6    Tmall.com]

pandas를 이용해서 가져오기


--------------------------------------------------------------------------------------------------------------

네이버 뮤직 7월 3째주 top 50까지 제목 가져오기

url = 'https://music.naver.com/listen/history/index.nhn?type=TOTAL_V2&year=2020&month=07&week=3'

html_music = requests.get(url).text
soup_music = BeautifulSoup(html_music, 'lxml')

title = soup_music.select('a._title span.ellipsis')  #id 는 #   . 은 class 로 표현(약속)
#print(title[0:6])

titles = [title_element.get_text()
          for title_element in title[0:]]

# print("[Top Music]")
# for k in range(len(title)):
#     print("{0}: {1}".format(k + 1, titles[k]))   <<- 긁어오기

title_dict = {'Title': titles}
df = pd.DataFrame(title_dict,
                  columns=['Title'], index=range(1,len(titles)+1))     <<- pandas 이용해서
print(df)



+ 가수 이름 가져오기

url = 'https://music.naver.com/listen/history/index.nhn?type=TOTAL_V2&year=2020&month=07&week=3'

html_artist = requests.get(url).text
soup_artist = BeautifulSoup(html_artist, 'lxml')

artist_name = soup_artist.select('a._artist span.ellipsis')
#print(artist_name[0:6])

artist_names = [artist_name_element.get_text().replace('\r', '').replace('\n', '').replace('\t', '')
                for artist_name_element in artist_name[0:]]
# print("[Artist Name]")
# for k in range(len(artist_name)):
#     print("{0}: {1}".format(k + 1, artist_names[k]))

artist_dict = {'Name': artist_names}
df = pd.DataFrame(artist_dict,
                  columns=['Name'], index=range(1, len(artist_names)+1))
print(df)


** 여기서 41등인 lady gaga 가 나오지 않는다. 다시 검사해서 적용하면
----------------------------------------------------------------------------------------------------------

다른 방식 +

url = 'https://music.naver.com/listen/history/index.nhn?type=TOTAL_V2&year=2020&month=07&week=3'

html_artist = requests.get(url).text
soup_artist = BeautifulSoup(html_artist, 'lxml')

artist_name = soup_artist.select('td._artist a')
#print(artist_name[0:6])

artist_names = [artist_name_element.get_text().strip()  <<- 간단하게 \r \n \t 없애는 방법
                for artist_name_element in artist_name[0:]]
# print("[Artist Name]")
# for k in range(len(artist_name)):
#     print("{0}: {1}".format(k + 1, artist_names[k]))

artist_dict = {'Name': artist_names}
df = pd.DataFrame(artist_dict,
                  columns=['Name'], index=range(1, len(artist_names)+1))

print(df)


*****2가지 결과물을 합치는 방법

for k in range(len(artist_names)):
    print("{0}: {1}/{2}".format(k+1, titles[k], artist_names[k]))


****** pandas 로 가져온 결과물은 약간 오른쪽 정렬로 되어있는듯한 느낌을 받는다. **파이참에서 보여지는건 의미가 없음
결국 데이터파일을 엑셀로 추출할거라 추출했을 당시 문제가 있다면 수정해야하지만, 그렇지 않을시에는 괜찮다.

** 주피터 노트북에서 DataFrame 으로 해보면 오른쪽 정렬로 되어서 나온다.

---------------------------------------------------------------------------------------------------------

전체를 이용해서 네이버 top music 50 위까지의 노래제목과 가수이름을 메모장에 저장해보자

file_name = ('D:/NaverMusicTop100.txt')  <<- 저장하고 싶은 장소 선정
f = open(file_name, 'w', encoding='utf-8')  <<- 파일, '읽기전용' encoding utf-8 유니코드 기반 언어표준

url = 'https://music.naver.com/listen/history/index.nhn?type=TOTAL_V2&year=2020&month=07&week=3'

html_music = requests.get(url).text
soup_music = BeautifulSoup(html_music, 'lxml')

title = soup_music.select('a._title span.ellipsis')  #id 는 #   . 은 class 로 표현(약속)
#print(title[0:6])

titles = [title_element.get_text()
          for title_element in title[0:]]

print("[Top Music]")
for k in range(len(title)):
    print("{0}: {1}".format(k + 1, titles[k]))

# title_dict = {'Title': titles}  <<- pandas 로 같은 결과 출력하기
# df = pd.DataFrame(title_dict,
#                   columns=['Title'], index=range(1,len(titles)+1))
# print(df)

**노래제목 추출



url = 'https://music.naver.com/listen/history/index.nhn?type=TOTAL_V2&year=2020&month=07&week=3'

html_artist = requests.get(url).text
soup_artist = BeautifulSoup(html_artist, 'lxml')

artist_name = soup_artist.select('td._artist a')
#print(artist_name[0:6])

artist_names = [artist_name_element.get_text().strip()
                for artist_name_element in artist_name[0:]]
print("[Artist Name]")
for k in range(len(artist_name)):
    print("{0}: {1}".format(k + 1, artist_names[k]))

# artist_dict = {'Name': artist_names}   <<pandas로 같은 결과 출력하기
# df = pd.DataFrame(artist_dict,
#                   columns=['Name'], index=range(1, len(artist_names)+1))
#
# print(df)

** 가수이름 추출



# for k in range(len(titles)):
#     print("{0}: {1}/{2}".format(k+1, titles[k], artist_names[k]))   <<- 노래제목/가수이름 출력

for k in range(len(artist_names)):
    f.write("{0}: {1}/{2}\n".format(k + 1, titles[k], artist_names[k]))   <<- 설정한 경로에 실제로 저장하는 코드


**** 경로에 가면 메모장으로 노래제목/가수이름이 정리되어있는걸 볼 수 있다.

--------------------------------------------------------------------------------------------------------------

#파이썬싸이트에서 파이썬로고 내려받아보기

url = 'https://www.python.org/static/img/python-logo.png'
html_image = requests.get(url)
image_file_name = os.path.basename(url)
folder = 'D:\ImageDownload'

if not os.path.exists(folder):
    os.makedirs(folder)

image_path = os.path.join(folder, image_file_name)

imageFile = open(image_path, 'wb')
# 이미지 데이터를 10000000 바이트씩 나눠서 저장
chunk_size = 1000000

for chunk in html_image.iter_content(chunk_size):
    imageFile.write(chunk)

imageFile.close()



--------------------------------------------------------------------------------------------------------------

# 사이트명, 저장폴더 지정


def get_image_url(url):
    html_image_url = requests.get(url).text
    soup_image_url = BeautifulSoup(html_image_url, "lxml")
    image_elements = soup_image_url.select(('picture img'))
    if image_elements != None:
        image_urls = []
        for image_element in image_elements:
            image_urls.append(image_element.get('data-src'))
        return image_urls
    else:
        return None

# 폴더를 지정해 이미지 주소에서 이미지 내려받기
def download_image(img_folder, img_url):
    if (img_url != None):
        html_image = requests.get(img_url)
        # os.path.basename(URL)는 웹사이트나 폴더가 포함된 파일명에서 파일명만 분리
        image_file = open(os.path.join(img_folder, os.path.basename(img_url)), 'wb')

        chunk_size = 1000000
        for chunk in html_image.iter_content(chunk_size):
            image_file.write(chunk)
            image_file.close()
        print("이미지 파일명: '{0}.내려받기 완료!".format(os.path.basename(img_url)))
    else:
        print("내려받을 이미지가 없습니다.")


url = 'https://picjumbo.com/free-stock-photos/nature/'
figure_folder = "D:\imagedownload"

picjumbo_image_urls = get_image_url(url) #이미지 파일의 주소 가져오기
num_of_download_image = 7 # 내려받을 이미지 개수 지정
# num_of_download_image = len(picjumbo_image_urls) # 전체 이미지를 내려받으려면
for k in range(num_of_download_image):
    download_image(figure_folder, picjumbo_image_urls[k])

print("===============================")
print("선택한 모든 이미지 내려받기 완료")

--------------------------------------------------------------------------------------------------------------

빅데이터 관련 프로젝트를 진행 중 내가 필요한 사진을 발견하게 되었다.
#크롤링이라는 것을 사용해서 이 모든걸 가져오겠다.

url = 'https://picjumbo.com/free-stock-photos/nature/'
# url 에는 내가 발견한 사진이 있는 사이트 주소가 들어가야 한다.

html_picjumbo_image = requests.get(url).text
# text 는 url 안에 있는 html 코드를 전부 불러올 수 있게 해준다.
# requests 라는 라이브러리는 get 모듈을 이용해서 인터넷에 연결해 url 에 있는 정보를 가져올 수 있는데,
# text 가 붙지 않으면, response[200] 이라는 성공했다는 문구만 보여준다.
# text 를 통해서 사이트에 있는 html 코드를 전부 가져올 수 있게 만들어준다.

soup_picjumbo_image = BeautifulSoup(html_picjumbo_image, "lxml")
# bs4 는 모듈들을 담고 있는게 라이브러리 Beautufulsoup 는 하나의 모듈, 도서관과 책이라고 생각할 것.
# BeautifulSoup은 html 코드를 Python이 이해하는 객체 구조로 변환하는 Parsing을 맡고 있고,
# 이 라이브러리를 이용해 우리는 제대로 된 '의미있는' 정보를 추출해 낼 수 있다.
# lxml 은 들어온 코드를 잘 이해할 수 있게 도와주는 모듈, Beautiful 은 ""(스트링) 에 있어도 확인해서 모듈로 인식하게 해준다.

nature_pic_tag_list = soup_picjumbo_image.select('picture img.image')
# 해당 사진파일에서 오른쪽 마우스 클릭으로 '검사' 에 들어가면 해당 사진의 html 코드를 볼 수 있다.
# 코드에서 공통적으로 사용되는 연결 구조를 찾아 작성한다. Picture 라는 큰 카테고리에서 img class(. 으로 표현)image 로 표현된다.
# picture img.image 에 해당되는 코드를 가져왔지만, 더 간단하게 표현해야 한다.
# 예를 들면 우리가 결과적으로 필요한 주소값만 필요하다. 더 정리할 필요가 있다.

nature_pic_url_list = []
# 먼저 정리된 주소값만 넣어줄 list 를 만든다.


for nature_pic_tag in nature_pic_tag_list:
    # 태그로 뽑아낸 코드들을 nature_pic_tag 에 하나씩 넣겠다.
    nature_pic_url = nature_pic_tag.get("data-src")
    # 뽑아낸 하나의 코드 중 get 메서드를 이용하여 data-src 부분(url)만 따로 빼내어 nature_pic_url 변수에 할당하겠다.
    nature_pic_url_list.append(nature_pic_url)
    # 할당된 url 을 nature_pic_url_list 리스트에 append 메서드로 하나씩 뒤에서부터 넣겠다.
print(nature_pic_url_list)
# url만 추출된다.

#하지만 아직 더 해야 할 일이 남아있다. 우리는 파일 이름을 따로 지어주지 않고, 본래 url 이 갖고 있는 이름의 마지막부분만
#빼내어 이름으로 만들어주고 싶다.


image_file_name_list = []
# 따로 이름만 다시 넣어주기 위한 리스트를 만든다.

for nature_pic_url in nature_pic_url_list:
    # url 이름이 들어있는 전체 리스트에서 하나씩 빼내어 nature_pic_url 에 넣겠다.
    image_file_name = os.path.basename(nature_pic_url)
    # os 라이브러리의 path 모듈에서 basename 함수를 이용해 nature_pic_url 에 있는 하나의 url 중 마지막에 / 전까지의
    # 이름을 빼내어 image_file_name 으로 할당하겠다.
    # 여기서 잠깐? 왜 이렇게 복잡하고 귀찮게 하는걸까? 사실 basename 함수는 리스트에 있는 요소들을 따로 하나씩 빼내어
    # 작업할 수 없다. 그렇기 때문에 다시 하나씩 빼내서 작업을 하게 만들었다는 사실
    image_file_name_list.append(image_file_name)
    # 할당된 파일 이름들을 image_file_name_list 에 뒤에서부터 하나씩 append 하여 넣어준다.


folder = 'D:\imagedownload'
# 이제 만들어진 이름과 진짜 크롤링하여 받을 폴더가 필요해졌다.
# 사실 이건 처음에 만들어도 상관은 없다.

if not os.path.exists(folder):
    os.makedirs(folder)
# 만약 만들고자 하는 폴더가 해당 자리에 있다면 if문은 작동하지 않고, 해당 폴더가 없다면
# 새로 하나 만들어준다.

# 이제 준비는 끝났다. 파일과 이름을 잘 넣어주기만 하면 된다.
# enumerate 함수는 첫번째는 인덱스 element, 두번째는 해당 리스트의 element 를 할당해준다.
# 밑에 있는 nature_pic_url_list[i] 의 i 를 위해 이렇게 for 문을 돌리는데,
# 만약 enumertate 없이 for문을 돌리면 첫 for 문에서 image_file_name 으로 결과값이,
# 들어가게 되고, 그대로 다음 for문에 들어간다.
# 사실 이름은 들어가게 되는 파일의 '이름' 일뿐이니, 파일이 없으면 의미가 없게 된다.
# 그래서 enumerate 를 써서 인덱스 값을 할당해주면서
# 두번째 for 문에서도 nature_pic_url_list 에 있는 (위에 보고 와라)
# 진짜 파일이 담겨있는 url 주소를 인덱스별로 가져올 수 있게 해준다는 것.

for i, image_file_name in enumerate(image_file_name_list):
    # enumerate 함수를 사용해서 인덱스 0번과 image_file_name_list 의 첫번째 인자가 image_file_name 으로 할당된다.
    image_path = os.path.join(folder, image_file_name) #join 메서드
    # os 라이브러리의 path 모듈에서 join 함수를 이용하여 folder 와 image_file_name 을 합한 결과를 image_path 에 할당한다.
    # 즉, folder 안에 image_file_name 이 들어간다.
    imageFile = open(image_path, 'wb')
    # image_path (들어갈 길) 이 있으니, 폴더를 열어 받을 준비를 할 수 있게 한다.
    # 넣어줘야 할 파일을 넣을 수 있게 준비하는 open write and binary
    chunk_size = 10000000
    # 특정 크기만큼 정해주지 않으면 제대로 작동하지 않을때도 있어서 chunk_size 를 정해준다.
    # 이렇게 하면, size 만큼 용량이 채워질때까지 기다렸다가 보내고, 다시 그 용량만큼 기다렸다가 보내는 작업을 반복한다.
    # 어느 단위로 파일을 넣어줄지 정하는 작업

    get_connetion_picture_url = requests.get(nature_pic_url_list[i])
    # requests 라이브러리는 인터넷과 연결시켜주며, get 함수를 통해 nature_pic_url_list[i] 의 i
    # 즉, 첫번째 인자를 get_connetion_picture_url 에 할당한다.
    for chunk in get_connetion_picture_url.iter_content(chunk_size):
        # nature_pic_url_list[i] 통해 할당된 get_connetion_picture_url 첫번째 인자를
        # iter_content(받은 content를 하나 하나 처리할 수 있게 해주는 컬렉션) 로 chunk_size 만큼 나누어
        # chunk 에 할당한다.
        imageFile.write(chunk)
        # 열어놓은 파일공간에 chunk 에서 받은 size 만큼의 url 을 write 적어넣는다 ** 넣는다.
    imageFile.close()
    # 전부 끝나면 닫는다.

---------------------------------------------------------------------------------------------------------------------------

#강사님 답안


def get_image_url(url):
    # url 을 인자값으로 받는 get_image_url 함수를 만든다.
    html_image_url = requests.get(url).text
    # requests 라이브러리 get 함수를 이용해 인터넷에 연결해 url 을 가져오는데,
    # text 를 이용해서 관련 html 코드를 전부 html_image_url 에 할당한다.
    soup_image_url = BeautifulSoup(html_image_url, "lxml")
    # BeautifulSoup 라이브러리로 html_image_url 에 있는 코드를 해석한다.
    # BeautifulSoup은 html 코드를 Python이 이해하는 객체 구조로 변환하는 Parsing을 맡고 있고,
    # 이 라이브러리를 이용해 우리는 제대로 된 '의미있는' 정보를 추출해 낼 수 있다.
    image_elements = soup_image_url.select(('picture img'))
    # '검사' 에서 얻을 수 있는 공통된 코드인자로 select 한 url 들을 image_elements 에 할당한다.
    if image_elements != None:
        # 만약 image_elements 가 None 이 아니라면
        image_urls = []
        # image_urls 라는 리스트를 만든다.
        for image_element in image_elements:
            # image_elements 의 url들을 image_element 에 하나씩 넣고
            image_urls.append(image_element.get('data-src'))
            #image_urls에 image_element 에서 get 메서드로 얻은 data-src 가 포함한 url 주소를 append 로 뒤에서부터 넣어준다.
        return image_urls
            # return 값으로 image_urls 준다.
    else:
        # 만약 image_elements 가 None 이라면
        return None
        # return 값 역시 None 이다.

def download_image(img_folder, img_url):
    # download_image 함수를 만들고 매개변수로 img_folder 와 img_url 을 할당한다.
    if (img_url != None):
        # 만약 img_url 이 None 이 아니라면
        html_image = requests.get(img_url)
        # requests로 인터넷에 연결하여 get 함수를 이용해 img_url 에 있는 url 을 html_image 에 할당한다.

        image_file = open(os.path.join(img_folder, os.path.basename(img_url)), 'wb')
        # os.path.basename(URL)는 웹사이트나 폴더가 포함된 파일명에서 파일명만 분리
        # os 라이브러리의 path 모듈, basename 을 이용해 img_url 의 url 중 파일명만 분리하고,
        # img_folder 와 join 함수로 분리된 파일명을 열린 공간(open) 에 즉, image_file 에 할당한다.

        chunk_size = 1000000
        # 보내줄 크기를 정한다.
        for chunk in html_image.iter_content(chunk_size):
            # img_url 에 있는 url 이 할당되어 있는 html_image 을 chunk_size 로 iterator 적용, 하나하나 분리해서
            # chunk 에 넣는다.
            image_file.write(chunk)
            # image_file 에 chunk 로 분리된 파일을 넣어준다. (*******여기서 파일은 이미 받아짐)
            image_file.close()
        print("이미지 파일명: '{0}.내려받기 완료!".format(os.path.basename(img_url)))
                                                # 내려받는거 파일명으로 알려주기
    else:
        print("내려받을 이미지가 없습니다.")

url = 'https://picjumbo.com/free-stock-photos/nature/'
figure_folder = "D:\imagedownload"
# 저장할 folder 정하기

picjumbo_image_urls = get_image_url(url) #이미지 파일의 주소 가져오기
num_of_download_image = 7 # 내려받을 이미지 개수 지정
# num_of_download_image = len(picjumbo_image_urls) # 전체 이미지를 내려받으려면
for k in range(num_of_download_image):
    # 7개의 이미지를 내려받아 k 에 하나씩 넣고
    download_image(figure_folder, picjumbo_image_urls[k])
    # 위에 만들어 놓은 download_image 함수 의 인자로 적용되어 실행됨
print("===============================")
print("선택한 모든 이미지 내려받기 완료")


